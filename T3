# DeepWiki Q&A with Code Context for Repository: ASTXRTYS/index



### Answer

Based on the LangGraph and LangSmith patterns in your codebase, here's a comprehensive tracing strategy for your handoff refinement loop:

## 1. **Structure Interrupt Payloads with Rich Iteration Metadata**

Each interrupt in your refinement loop should carry detailed metadata about the iteration state. Based on the HITL patterns, extend your `HandoffActionArgs` to include: index:13-34 

```typescript
HandoffActionArgs = {
  name: "handoff_summary",
  args: {
    summary_json: {...},
    summary_md: "...",
    // NEW: Iteration tracking
    iteration: number,
    previous_feedback: string[] | None,
    refinement_history: [{
      iteration: int,
      feedback: str,
      timestamp: str
    }] | None,
    is_refinement: bool
  },
  description: "..."  // Dynamically updated to reflect iteration
}
```

The `interrupt()` function supports custom IDs that you can use to track iterations: index:401-424 

Generate unique interrupt IDs per iteration: `f"handoff_summary_{thread_id}_iter_{iteration}"` using `Interrupt.from_ns()`: index:183-185 

## 2. **Annotate Each Summarization LLM Call with Iteration Context**

The key to traceability is ensuring each summarization LLM call has rich metadata. When invoking your model in `generate_handoff_summary()`, construct a `RunnableConfig` that includes iteration-specific metadata:

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    run_name=f"handoff_summary_iteration_{iteration}",
    metadata={
        "handoff_iteration": iteration,
        "feedback": user_feedback if iteration > 0 else None,
        "refinement_request": True if iteration > 0 else False,
        "parent_thread_id": parent_thread_id,
        "assistant_id": assistant_id,
        "previous_summary": previous_summary_snippet if iteration > 0 else None
    },
    tags=[
        f"handoff-refinement",
        f"iteration-{iteration}",
        "handoff-summary"
    ]
)

response = self.model.invoke(
    refinement_prompt,
    config=config
)
```

This pattern is documented for LangChain tracing: index:1-50 

The metadata will be attached to the run and inherited by child runs: index:1-100 

## 3. **Build Feedback-Aware Prompts with Visible Structure**

Your refinement prompt should explicitly show how user feedback modifies the base prompt. Build it as a multi-part prompt:

```python
def build_refinement_prompt(
    messages: list[AnyMessage],
    user_feedback: str,
    previous_summary: str | None,
    iteration: int
) -> str:
    base_prompt = DEFAULT_SUMMARY_PROMPT  # From summarization.py
    
    refinement_section = f"""

<refinement_context>
<iteration>{iteration}</iteration>
<user_feedback>
{user_feedback}
</user_feedback>
<previous_summary>
{previous_summary}
</previous_summary>
</refinement_context>

<refinement_instructions>
The user has provided feedback on the previous summary. 
Please generate a NEW summary that addresses their specific concerns.

User's feedback: "{user_feedback}"

Focus on incorporating their guidance while maintaining the essential context.
</refinement_instructions>
"""
    
    return base_prompt + refinement_section + "\n\n<messages>\nMessages to summarize:\n{messages}\n</messages>"
```

Reference the base summarization prompt structure: index:26-52 

## 4. **Implement Iteration-Aware Middleware Loop**

Your `HandoffSummarizationMiddleware.after_model` should detect refinement requests and loop. The pattern follows how `interrupt()` handles resume values: index:491-524 

```python
class HandoffSummarizationMiddleware(AgentMiddleware):
    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        # Check for handoff_requested flag
        if not state.get("handoff_requested"):
            return None
        
        # Get iteration state from middleware-private state
        iteration = state.get("_handoff_iteration", 0)
        previous_summary = state.get("_handoff_previous_summary")
        refinement_feedback = state.get("_handoff_feedback")
        
        # Generate summary (with feedback if iteration > 0)
        summary = self._generate_summary_with_tracing(
            model=self.model,
            messages=state["messages"],
            iteration=iteration,
            feedback=refinement_feedback,
            previous_summary=previous_summary,
            runtime=runtime
        )
        
        # Build interrupt payload
        action_request = ActionRequest(
            name="handoff_summary",
            args={
                "summary_json": summary.to_dict(),
                "summary_md": summary.to_markdown(),
                "iteration": iteration,
                "refinement_history": state.get("_handoff_refinement_history", []),
            },
            description=self._build_iteration_description(iteration, refinement_feedback)
        )
        
        review_config = ReviewConfig(
            action_name="handoff_summary",
            allowed_decisions=["approve", "reject", "refine"]  # NEW: "refine" instead of "edit"
        )
        
        # Emit interrupt with iteration-specific ID
        interrupt_id = f"handoff_summary_{runtime.config['thread_id']}_iter_{iteration}"
        hitl_response: HITLResponse = interrupt(
            HITLRequest(
                action_requests=[action_request],
                review_configs=[review_config]
            )
        )
        
        # Process decision
        decision = hitl_response["decisions"][0]
        
        if decision["type"] == "refine":
            # Extract feedback and loop back
            feedback = decision["edited_action"]["args"]["feedback"]
            return {
                "_handoff_iteration": iteration + 1,
                "_handoff_feedback": feedback,
                "_handoff_previous_summary": summary.to_markdown(),
                "_handoff_refinement_history": [
                    *state.get("_handoff_refinement_history", []),
                    {
                        "iteration": iteration,
                        "feedback": feedback,
                        "timestamp": datetime.now().isoformat()
                    }
                ],
                # Keep handoff_requested=True to trigger another after_model call
                "handoff_requested": True,
                # Use jump_to to force another model->after_model cycle
                "jump_to": "model"
            }
        
        elif decision["type"] == "approve":
            return {
                "handoff_approved": True,
                "handoff_decision": decision,
                "handoff_summary": summary,
                "_handoff_iteration": None,  # Clear state
                # ... rest of approval logic
            }
```

The middleware pattern and state updates follow the documented approach: index:205-249 

## 5. **Trace Summarization Calls with Explicit Run Names**

In your `_generate_summary_with_tracing` helper, use the `@traceable` decorator or explicit run tree creation to ensure each iteration is a separate traceable span:

```python
def _generate_summary_with_tracing(
    self,
    model: BaseChatModel,
    messages: list[AnyMessage],
    iteration: int,
    feedback: str | None,
    previous_summary: str | None,
    runtime: Runtime
) -> HandoffSummary:
    # Build prompt with feedback
    if iteration == 0:
        prompt = self._build_initial_prompt(messages)
    else:
        prompt = self._build_refinement_prompt(
            messages, feedback, previous_summary, iteration
        )
    
    # Create config with rich metadata
    config = RunnableConfig(
        run_name=f"generate_handoff_summary_iter_{iteration}",
        metadata={
            "handoff_iteration": iteration,
            "has_feedback": feedback is not None,
            "feedback_preview": feedback[:100] if feedback else None,
            "summary_type": "refinement" if iteration > 0 else "initial",
            "parent_thread_id": runtime.config.get("configurable", {}).get("thread_id"),
        },
        tags=[
            "handoff-summary",
            f"iteration-{iteration}",
            "refinement" if iteration > 0 else "initial"
        ]
    )
    
    # Invoke with tracing
    response = model.invoke(prompt, config=config)
    
    # Parse and return
    return self._parse_summary_response(response.content)
```

The `model.invoke()` call will automatically create a traced run with your metadata: index:221-234 

## 6. **Structure Decision Records for Auditability**

Your `HandoffDecisionRecord` should capture the full refinement trail:

```python
@dataclass
class HandoffDecisionRecord:
    decision_type: Literal["approve", "reject", "refine"]
    iteration: int
    feedback: str | None
    timestamp: str
    summary_snapshot: str  # The summary at this iteration
    
    # Audit trail
    total_iterations: int
    refinement_history: list[dict]  # All previous feedback + summaries
```

When normalizing decisions in your middleware: index:238-278 

## 7. **Link Interrupts to Middleware via Runtime Context**

Use the `Runtime` object to maintain context across iterations. The runtime is passed to all middleware hooks: index:84-96 

Store iteration state in ephemeral state attributes that won't leak into the schema: index:159-176 

## 8. **CLI Handling of Refinement Loop**

In your `execute_task`, when you detect a "refine" decision kind, construct the resume payload with feedback:

```python
# In execute_task HITL handling
if decision_kind == "refine":
    feedback = prompt_for_refinement_feedback()  # New UI function
    
    decisions.append({
        "type": "refine",
        "edited_action": {
            "name": "handoff_summary",
            "args": {
                "feedback": feedback,
                "iteration": action_request["args"]["iteration"] + 1
            }
        }
    })
    
    hitl_response = {"decisions": decisions}
    stream_input = Command(resume={interrupt_id: hitl_response})
    # Loop continues, middleware will re-invoke after_model
```

The `Command(resume=...)` pattern with interrupt ID mapping: index:348-398 

## 9. **Auditing in LangSmith**

With this structure, each handoff refinement will create a trace hierarchy:

```
Run: agent_execution [trace_id=xxx]
├─ Run: handoff_tool_middleware.after_model [iteration=0]
│  ├─ Run: generate_handoff_summary_iter_0
│  │  └─ Run: ChatAnthropic.invoke [prompt shows base summary request]
│  └─ Interrupt: handoff_summary_xxx_iter_0
├─ Resume: handoff_summary_xxx_iter_0 [decision=refine, feedback="focus on repro steps"]
├─ Run: handoff_tool_middleware.after_model [iteration=1]
│  ├─ Run: generate_handoff_summary_iter_1
│  │  └─ Run: ChatAnthropic.invoke [prompt shows feedback + refinement instructions]
│  └─ Interrupt: handoff_summary_xxx_iter_1
├─ Resume: handoff_summary_xxx_iter_1 [decision=approve]
└─ Run: apply_handoff_acceptance
```

Query your traces using metadata filters: index:100-200 

## Notes

**Key Tracing Principles:**

1. **Unique Run Names Per Iteration**: Use `run_name=f"generate_handoff_summary_iter_{iteration}"` so each refinement shows as a distinct span in LangSmith.

2. **Rich Metadata on Every LLM Call**: Include `handoff_iteration`, `feedback`, `refinement_request` in the config's metadata dict. This makes filtering and analysis trivial.

3. **Interrupt IDs as Iteration Keys**: Generate deterministic interrupt IDs like `f"handoff_summary_{thread_id}_iter_{iteration}"` to correlate interrupts with their corresponding middleware invocations.

4. **Prompt Structure Visibility**: Build refinement prompts that explicitly show `<refinement_context>` and `<user_feedback>` sections so the trace shows how feedback mutates the prompt.

5. **State-Based Loop Control**: Use ephemeral state fields (`_handoff_iteration`, `_handoff_feedback`) with `PrivateStateAttr` annotation to maintain loop state without polluting your schema.

6. **Resume with Structured Feedback**: The `Command(resume={interrupt_id: {"decisions": [{"type": "refine", "edited_action": {"args": {"feedback": "..."}}}]}})` pattern passes feedback back to middleware cleanly.

7. **Audit Trail in Decision Records**: Capture the full `refinement_history` in your decision records so you can reconstruct the evolution of summaries.

The interrupt/resume mechanism is designed for exactly this use case—iterative refinement loops with human feedback. Each iteration creates a separate interrupt, the middleware processes the resume value, and the cycle continues until approval. All of this is automatically traced if you use `RunnableConfig` with metadata on your LLM calls.


### Citations


**File:** langchain-master/libs/langchain_v1/langchain/agents/middleware/human_in_the_loop.py (L13-34)

```python
class Action(TypedDict):
    """Represents an action with a name and args."""

    name: str
    """The type or name of action being requested (e.g., `'add_numbers'`)."""

    args: dict[str, Any]
    """Key-value pairs of args needed for the action (e.g., `{"a": 1, "b": 2}`)."""


class ActionRequest(TypedDict):
    """Represents an action request with a name, args, and description."""

    name: str
    """The name of the action being requested."""

    args: dict[str, Any]
    """Key-value pairs of args needed for the action (e.g., `{"a": 1, "b": 2}`)."""

    description: NotRequired[str]
    """The description of the action to be reviewed."""

```

**File:** langchain-master/libs/langchain_v1/langchain/agents/middleware/human_in_the_loop.py (L238-278)

```python
    def _process_decision(
        self,
        decision: Decision,
        tool_call: ToolCall,
        config: InterruptOnConfig,
    ) -> tuple[ToolCall | None, ToolMessage | None]:
        """Process a single decision and return the revised tool call and optional tool message."""
        allowed_decisions = config["allowed_decisions"]

        if decision["type"] == "approve" and "approve" in allowed_decisions:
            return tool_call, None
        if decision["type"] == "edit" and "edit" in allowed_decisions:
            edited_action = decision["edited_action"]
            return (
                ToolCall(
                    type="tool_call",
                    name=edited_action["name"],
                    args=edited_action["args"],
                    id=tool_call["id"],
                ),
                None,
            )
        if decision["type"] == "reject" and "reject" in allowed_decisions:
            # Create a tool message with the human's text response
            content = decision.get("message") or (
                f"User rejected the tool call for `{tool_call['name']}` with id {tool_call['id']}"
            )
            tool_message = ToolMessage(
                content=content,
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
                status="error",
            )
            return tool_call, tool_message
        msg = (
            f"Unexpected human decision: {decision}. "
            f"Decision type '{decision.get('type')}' "
            f"is not allowed for tool '{tool_call['name']}'. "
            f"Expected one of {allowed_decisions} based on the tool's configuration."
        )
        raise ValueError(msg)
```

**File:** langgraph-main/libs/langgraph/langgraph/types.py (L183-185)

```python
    @classmethod
    def from_ns(cls, value: Any, ns: str) -> Interrupt:
        return cls(value=value, id=xxh3_128_hexdigest(ns.encode()))
```

**File:** langgraph-main/libs/langgraph/langgraph/types.py (L348-398)

```python
@dataclass(**_DC_KWARGS)
class Command(Generic[N], ToolOutputMixin):
    """One or more commands to update the graph's state and send messages to nodes.

    Args:
        graph: Graph to send the command to. Supported values are:

            - `None`: the current graph
            - `Command.PARENT`: closest parent graph
        update: Update to apply to the graph's state.
        resume: Value to resume execution with. To be used together with [`interrupt()`][langgraph.types.interrupt].
            Can be one of the following:

            - Mapping of interrupt ids to resume values
            - A single value with which to resume the next interrupt
        goto: Can be one of the following:

            - Name of the node to navigate to next (any node that belongs to the specified `graph`)
            - Sequence of node names to navigate to next
            - `Send` object (to execute a node with the input provided)
            - Sequence of `Send` objects
    """

    graph: str | None = None
    update: Any | None = None
    resume: dict[str, Any] | Any | None = None
    goto: Send | Sequence[Send | N] | N = ()

    def __repr__(self) -> str:
        # get all non-None values
        contents = ", ".join(
            f"{key}={value!r}" for key, value in asdict(self).items() if value
        )
        return f"Command({contents})"

    def _update_as_tuples(self) -> Sequence[tuple[str, Any]]:
        if isinstance(self.update, dict):
            return list(self.update.items())
        elif isinstance(self.update, (list, tuple)) and all(
            isinstance(t, tuple) and len(t) == 2 and isinstance(t[0], str)
            for t in self.update
        ):
            return self.update
        elif keys := get_cached_annotated_keys(type(self.update)):
            return get_update_as_tuples(self.update, keys)
        elif self.update is not None:
            return [("__root__", self.update)]
        else:
            return []

    PARENT: ClassVar[Literal["__parent__"]] = "__parent__"
```

**File:** langgraph-main/libs/langgraph/langgraph/types.py (L401-424)

```python
def interrupt(value: Any) -> Any:
    """Interrupt the graph with a resumable exception from within a node.

    The `interrupt` function enables human-in-the-loop workflows by pausing graph
    execution and surfacing a value to the client. This value can communicate context
    or request input required to resume execution.

    In a given node, the first invocation of this function raises a `GraphInterrupt`
    exception, halting execution. The provided `value` is included with the exception
    and sent to the client executing the graph.

    A client resuming the graph must use the [`Command`][langgraph.types.Command]
    primitive to specify a value for the interrupt and continue execution.
    The graph resumes from the start of the node, **re-executing** all logic.

    If a node contains multiple `interrupt` calls, LangGraph matches resume values
    to interrupts based on their order in the node. This list of resume values
    is scoped to the specific task executing the node and is not shared across tasks.

    To use an `interrupt`, you must enable a checkpointer, as the feature relies
    on persisting the graph state.

    !!! example

```

**File:** langgraph-main/libs/langgraph/langgraph/types.py (L491-524)

```python
    from langgraph._internal._constants import (
        CONFIG_KEY_CHECKPOINT_NS,
        CONFIG_KEY_SCRATCHPAD,
        CONFIG_KEY_SEND,
        RESUME,
    )
    from langgraph.config import get_config
    from langgraph.errors import GraphInterrupt

    conf = get_config()["configurable"]
    # track interrupt index
    scratchpad = conf[CONFIG_KEY_SCRATCHPAD]
    idx = scratchpad.interrupt_counter()
    # find previous resume values
    if scratchpad.resume:
        if idx < len(scratchpad.resume):
            conf[CONFIG_KEY_SEND]([(RESUME, scratchpad.resume)])
            return scratchpad.resume[idx]
    # find current resume value
    v = scratchpad.get_null_resume(True)
    if v is not None:
        assert len(scratchpad.resume) == idx, (scratchpad.resume, idx)
        scratchpad.resume.append(v)
        conf[CONFIG_KEY_SEND]([(RESUME, scratchpad.resume)])
        return v
    # no resume value found
    raise GraphInterrupt(
        (
            Interrupt.from_ns(
                value=value,
                ns=conf[CONFIG_KEY_CHECKPOINT_NS],
            ),
        )
    )
```

**File:** langsmith-docs-main/docs/observability/how_to_guides/trace_with_langchain.mdx (L1-50)

```text
---
sidebar_position: 14
---

import {
  LangChainInstallationCodeTabs,
  LangChainQuickStartCodeTabs,
  ConfigureLangChainEnvironmentCodeTabs,
} from "@site/src/components/QuickStart";

import {
  CodeTabs,
  TypeScriptBlock,
  PythonBlock,
  typescript,
  python,
} from "@site/src/components/InstructionsWithCode";

# Trace with `LangChain` (Python and JS/TS)

LangSmith integrates seamlessly with LangChain ([Python](https://python.langchain.com/) and [JS](https://js.langchain.com/docs/get_started/introduction)), the popular open-source framework for building LLM applications.

## Installation

Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).

For a full list of packages available, see the [LangChain Python docs](https://python.langchain.com/docs/integrations/platforms/) and [LangChain JS docs](https://js.langchain.com/docs/integrations/platforms/).

<LangChainInstallationCodeTabs />

## Quick start

### 1. Configure your environment

<ConfigureLangChainEnvironmentCodeTabs />

### 2. Log a trace

No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.

<LangChainQuickStartCodeTabs />

### 3. View your trace

By default, the trace will be logged to the project with the name `default`. An example of a trace logged using the above code is made public and can be viewed [here](https://smith.langchain.com/public/e6a46eb2-d785-4804-a1e3-23f167a04300/r).

![](./static/langchain_trace.png)

## Trace selectively

```

**File:** langsmith-docs-main/docs/observability/how_to_guides/add_metadata_tags.mdx (L1-200)

```text
---
sidebar_position: 5
---

import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
} from "@site/src/components/InstructionsWithCode";

# Add metadata and tags to traces

LangSmith supports sending arbitrary metadata and tags along with traces.

Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.

Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID.
For more information on tags and metadata, see the [Concepts](/observability/concepts#tags) page. For information on how to query traces and runs by metadata and tags, see the [Filter traces in the application](./filter_traces_in_application) page.

<CodeTabs
  tabs={[
    PythonBlock(`import openai
import langsmith as ls
from langsmith.wrappers import wrap_openai
client = openai.Client()\n
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]\n
# highlight-next-line
# You can set metadata & tags **statically** when decorating a function
# Use the @traceable decorator with tags and metadata
# Ensure that the LANGSMITH_TRACING environment variables are set for @traceable to work
@ls.traceable(
    run_type="llm",
    name="OpenAI Call Decorator",
    # highlight-next-line
    tags=["my-tag"],
    # highlight-next-line
    metadata={"my-key": "my-value"}
)
def call_openai(
    messages: list[dict], model: str = "gpt-4o-mini"
) -> str:
    # You can also dynamically set metadata on the parent run:
    # highlight-next-line
    rt = ls.get_current_run_tree()
    # highlight-next-line
    rt.metadata["some-conditional-key"] = "some-val"
    # highlight-next-line
    rt.tags.extend(["another-tag"])
    return client.chat.completions.create(
        model=model,
        messages=messages,
    ).choices[0].message.content\n
call_openai(
    messages,
    # To add at **invocation time**, when calling the function.
    # via the langsmith_extra parameter
    # highlight-next-line
    langsmith_extra={"tags": ["my-other-tag"], "metadata": {"my-other-key": "my-value"}}
)\n
# Alternatively, you can use the context manager
with ls.trace(
        name="OpenAI Call Trace",
        run_type="llm",
        inputs={"messages": messages},
        # highlight-next-line
        tags=["my-tag"],
        # highlight-next-line
        metadata={"my-key": "my-value"},
    ) as rt:
        chat_completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
        )
        rt.metadata["some-conditional-key"] = "some-val"
        rt.end(outputs={"output": chat_completion})
\n
# You can use the same techniques with the wrapped client
patched_client = wrap_openai(
    # highlight-next-line
    client, tracing_extra={"metadata": {"my-key": "my-value"}, "tags": ["a-tag"]}
)
chat_completion = patched_client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages,
    langsmith_extra={
        # highlight-next-line
        "tags": ["my-other-tag"],
        # highlight-next-line
        "metadata": {"my-other-key": "my-value"},
    },
)
`),
    TypeScriptBlock(`import OpenAI from "openai";
import { traceable, getCurrentRunTree } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";

const client = wrapOpenAI(new OpenAI());

const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
{ role: "system", content: "You are a helpful assistant." },
{ role: "user", content: "Hello!" },
];

const traceableCallOpenAI = traceable(
async (messages: OpenAI.Chat.ChatCompletionMessageParam[]) => {
const completion = await client.chat.completions.create({
model: "gpt-4o-mini",
messages,
});
const runTree = getCurrentRunTree();
runTree.extra.metadata = {
...runTree.extra.metadata,
someKey: "someValue",
};
runTree.tags = [...(runTree.tags ?? []), "runtime-tag"];
return completion.choices[0].message.content;
},
{
run_type: "llm",
name: "OpenAI Call Traceable",
tags: ["my-tag"],
metadata: { "my-key": "my-value" },
}
);
// Call the traceable function
await traceableCallOpenAI(messages);
`),
]}
groupId="client-language"
```

**File:** langchain-master/libs/langchain_v1/langchain/agents/middleware/summarization.py (L26-52)

```python
DEFAULT_SUMMARY_PROMPT = """<role>
Context Extraction Assistant
</role>

<primary_objective>
Your sole objective in this task is to extract the highest quality/most relevant context from the conversation history below.
</primary_objective>

<objective_information>
You're nearing the total number of input tokens you can accept, so you must extract the highest quality/most relevant pieces of information from your conversation history.
This context will then overwrite the conversation history presented below. Because of this, ensure the context you extract is only the most important information to your overall goal.
</objective_information>

<instructions>
The conversation history below will be replaced with the context you extract in this step. Because of this, you must do your very best to extract and record all of the most important context from the conversation history.
You want to ensure that you don't repeat any actions you've already completed, so the context you extract from the conversation history should be focused on the most important information to your overall goal.
</instructions>

The user will message you with the full message history you'll be extracting context from, to then replace. Carefully read over it all, and think deeply about what information is most important to your overall goal that should be saved:

With all of this in mind, please carefully read over the entire conversation history, and extract the most important and relevant context to replace it so that you can free up space in the conversation history.
Respond ONLY with the extracted context. Do not include any additional information, or text before or after the extracted context.

<messages>
Messages to summarize:
{messages}
</messages>"""  # noqa: E501
```

**File:** langchain-master/libs/langchain_v1/langchain/agents/middleware/summarization.py (L221-234)

```python
    def _create_summary(self, messages_to_summarize: list[AnyMessage]) -> str:
        """Generate summary for the given messages."""
        if not messages_to_summarize:
            return "No previous conversation history."

        trimmed_messages = self._trim_messages_for_summary(messages_to_summarize)
        if not trimmed_messages:
            return "Previous conversation was too long to summarize."

        try:
            response = self.model.invoke(self.summary_prompt.format(messages=trimmed_messages))
            return cast("str", response.content).strip()
        except Exception as e:  # noqa: BLE001
            return f"Error generating summary: {e!s}"
```

**File:** langchain-master/libs/langchain_v1/langchain/agents/middleware/types.py (L84-96)

```python
class ModelRequest:
    """Model request information for the agent."""

    model: BaseChatModel
    system_prompt: str | None
    messages: list[AnyMessage]  # excluding system prompt
    tool_choice: Any | None
    tools: list[BaseTool | dict]
    response_format: ResponseFormat | None
    state: AgentState
    runtime: Runtime[ContextT]  # type: ignore[valid-type]
    model_settings: dict[str, Any] = field(default_factory=dict)

```

**File:** langchain-master/libs/langchain_v1/langchain/agents/middleware/types.py (L159-176)

```python
class OmitFromSchema:
    """Annotation used to mark state attributes as omitted from input or output schemas."""

    input: bool = True
    """Whether to omit the attribute from the input schema."""

    output: bool = True
    """Whether to omit the attribute from the output schema."""


OmitFromInput = OmitFromSchema(input=True, output=False)
"""Annotation used to mark state attributes as omitted from input schema."""

OmitFromOutput = OmitFromSchema(input=False, output=True)
"""Annotation used to mark state attributes as omitted from output schema."""

PrivateStateAttr = OmitFromSchema(input=True, output=True)
"""Annotation used to mark state attributes as purely internal for a given middleware."""
```

**File:** langchain-master/libs/langchain_v1/langchain/agents/middleware/types.py (L205-249)

```python
class AgentMiddleware(Generic[StateT, ContextT]):
    """Base middleware class for an agent.

    Subclass this and implement any of the defined methods to customize agent behavior
    between steps in the main agent loop.
    """

    state_schema: type[StateT] = cast("type[StateT]", AgentState)
    """The schema for state passed to the middleware nodes."""

    tools: list[BaseTool]
    """Additional tools registered by the middleware."""

    @property
    def name(self) -> str:
        """The name of the middleware instance.

        Defaults to the class name, but can be overridden for custom naming.
        """
        return self.__class__.__name__

    def before_agent(self, state: StateT, runtime: Runtime[ContextT]) -> dict[str, Any] | None:
        """Logic to run before the agent execution starts."""

    async def abefore_agent(
        self, state: StateT, runtime: Runtime[ContextT]
    ) -> dict[str, Any] | None:
        """Async logic to run before the agent execution starts."""

    def before_model(self, state: StateT, runtime: Runtime[ContextT]) -> dict[str, Any] | None:
        """Logic to run before the model is called."""

    async def abefore_model(
        self, state: StateT, runtime: Runtime[ContextT]
    ) -> dict[str, Any] | None:
        """Async logic to run before the model is called."""

    def after_model(self, state: StateT, runtime: Runtime[ContextT]) -> dict[str, Any] | None:
        """Logic to run after the model is called."""

    async def aafter_model(
        self, state: StateT, runtime: Runtime[ContextT]
    ) -> dict[str, Any] | None:
        """Async logic to run after the model is called."""

```

