# DeepWiki Q&A with Code Context for Repository: ASTXRTYS/index



### Answer

Based on the LangGraph, LangChain, and DeepAgents codebases, I can provide you with a comprehensive architectural approach for implementing this model-in-the-loop refinement feature. Here's a modular, elegant solution with excellent LangSmith traceability:

## Core Architecture Pattern: Interrupt Loop with Refinement

The foundational pattern you need is the **validation/refinement interrupt loop** demonstrated in LangGraph's test suite. This pattern shows how a single node can loop multiple times, calling `interrupt()` on each iteration, validating the response, and continuing until acceptance: index:4875-4892 

The key insight here is that **the node re-executes from the beginning** on each resume, but `interrupt()` returns the resume value instead of pausing again. This allows you to implement a refinement loop entirely within your middleware's `after_model` hook.

## Middleware Design: Stateful Refinement Loop

Your `HandoffSummarizationMiddleware.after_model` should implement a loop that:
1. Checks if there's a pending refinement (from state or resume data)
2. Builds the appropriate prompt (initial or refinement)
3. Calls the LLM
4. Emits an interrupt
5. On resume, extracts feedback and loops back

The resume matching behavior is critical: index:401-422 

This means your middleware can contain a `for` loop that iterates up to a max refinement count, with each iteration calling `interrupt()` and processing the resumed feedback.

## Prompt Engineering: Refine Pattern

For building refinement prompts that incorporate feedback, use the **refine chain pattern** from LangChain. This pattern takes an existing summary and new context (your feedback) to produce a refined output: index:3-13 

Your refinement prompt should follow this structure:
```
REFINEMENT_PROMPT = """
You previously generated this summary:
{existing_summary}

The user provided this feedback:
{feedback}

Original conversation context:
{messages}

Refine the summary based on the feedback. If the feedback suggests no useful changes, return the original summary.
"""
```

For fact-checking and quality-driven refinement (optional enhancement), see: index:78-108 

## HITL Decision Structure

Structure your decisions to align with the existing HITL patterns in the codebase. The decision types should extend the base pattern: index:36-92 

For your handoff refinement, extend this with:
- `EditDecision` with `edited_action.args.feedback` for refinement requests
- `edited_action.args.iteration` to track refinement count
- `edited_action.args.previous_summary` to maintain history

Your `HandoffActionArgs` should include:
```python
class HandoffActionArgs(TypedDict):
    title: str
    summary_tldr: str
    summary_body: str
    iteration: int  # Refinement iteration count
    feedback_history: list[str]  # All feedback from previous iterations
    previous_summary: NotRequired[str]  # Previous draft for reference
```

## LangSmith Traceability

For excellent debugging in LangSmith, leverage the metadata and tags system to track iterations: index:56-93 

In your `generate_handoff_summary` function, wrap each LLM call with metadata:

```python
@traceable(name="generate_handoff_summary")
def generate_handoff_summary(
    model, 
    messages, 
    assistant_id, 
    parent_thread_id,
    feedback: str | None = None,
    previous_summary: str | None = None,
    iteration: int = 0,
):
    # Use langsmith_extra to add iteration metadata
    langsmith_extra = {
        "metadata": {
            "handoff_iteration": iteration,
            "has_feedback": feedback is not None,
            "refinement_type": "initial" if iteration == 0 else "refinement",
            "parent_thread_id": parent_thread_id,
        },
        "tags": [
            "handoff_summary",
            f"iteration_{iteration}",
            "refinement" if feedback else "initial"
        ]
    }
    
    # Build prompt based on whether this is initial or refinement
    if iteration == 0:
        prompt = DEFAULT_SUMMARY_PROMPT.format(messages=messages)
    else:
        prompt = REFINEMENT_PROMPT.format(
            existing_summary=previous_summary,
            feedback=feedback,
            messages=messages
        )
    
    response = model.invoke(prompt, config={"metadata": langsmith_extra["metadata"]})
    return response.content
```

The metadata merging happens automatically: index:1-50 

## Middleware Implementation Pattern

Your `HandoffSummarizationMiddleware.after_model` should follow this structure:

```python
def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    if not state.get("handoff_requested"):
        return None
    
    # Extract refinement state from previous resume
    refinement_state = self._get_refinement_state(state)
    iteration = refinement_state.get("iteration", 0)
    feedback_history = refinement_state.get("feedback_history", [])
    previous_summary = refinement_state.get("previous_summary")
    
    # Max iterations check
    if iteration >= self.max_refinement_iterations:
        return {"handoff_decision": "reject", "handoff_approved": False}
    
    # Generate summary (initial or refined)
    current_feedback = refinement_state.get("current_feedback")
    summary = self._generate_summary_with_tracing(
        messages=state["messages"],
        feedback=current_feedback,
        previous_summary=previous_summary,
        iteration=iteration
    )
    
    # Build HandoffActionArgs with iteration metadata
    action_args = HandoffActionArgs(
        title=self._extract_title(summary),
        summary_tldr=self._extract_tldr(summary),
        summary_body=summary,
        iteration=iteration,
        feedback_history=feedback_history,
        previous_summary=previous_summary,
    )
    
    # Emit interrupt with ReviewConfig
    hitl_request = HITLRequest(
        action_requests=[ActionRequest(
            name="handoff_summary",
            args=action_args,
            description=f"Handoff Summary (Iteration {iteration + 1})"
        )],
        review_configs=[ReviewConfig(
            action_name="handoff_summary",
            allowed_decisions=["approve", "edit", "reject"]
        )]
    )
    
    hitl_response: HITLResponse = interrupt(hitl_request)
    decision = hitl_response["decisions"][0]
    
    # Process decision
    if decision["type"] == "approve":
        return {"handoff_decision": decision, "handoff_approved": True}
    elif decision["type"] == "reject":
        return {"handoff_decision": decision, "handoff_approved": False}
    elif decision["type"] == "edit":
        # Extract feedback and loop again
        edited_action = decision["edited_action"]
        new_feedback = edited_action["args"].get("feedback", "")
        
        # Update refinement state for next iteration
        return {
            "_refinement_state": {
                "iteration": iteration + 1,
                "feedback_history": feedback_history + [new_feedback],
                "previous_summary": summary,
                "current_feedback": new_feedback,
            },
            "handoff_requested": True,  # Keep loop alive
        }
```

The interrupt documentation shows how this works: index:20-86 

## CLI Integration for Both Flows

For the **agent-initiated flow**, your CLI's `execute_task` already handles interrupts. Extend `prompt_handoff_decision` to include a "Refine" option that prompts for natural language feedback:

```python
def prompt_handoff_decision(proposal: HandoffProposal, preview_only: bool = False):
    # ... existing rendering code ...
    
    choices = ["Accept", "Refine", "Edit", "Decline"]
    if preview_only:
        choices.append("Preview")
    
    choice = questionary.select("Decision:", choices=choices).ask()
    
    if choice == "Refine":
        feedback = questionary.text(
            "What would you like to change? (natural language)",
            multiline=True
        ).ask()
        
        return {
            "type": "edit",
            "edited_action": {
                "name": "handoff_summary",
                "args": {
                    **proposal.args,
                    "feedback": feedback,
                }
            }
        }
    # ... handle other choices ...
```

For the **manual /handoff flow**, refactor to use the same middleware path. Instead of calling `generate_handoff_summary` directly, inject the handoff request into state and let the graph execute:

```python
def handle_handoff_command(args, agent, session_state):
    # Set up handoff request in state
    current_state = await agent.aget_state(config)
    
    # Trigger handoff via state update
    result = agent.invoke(
        Command(update={"handoff_requested": True, "handoff_manual": True}),
        config=config
    )
    
    # Now the middleware handles it, with interrupt loop support
    # CLI handles interrupts as usual through execute_task loop
```

This unifies both paths through the same middleware refinement loop.

## State Management

Store refinement state in a dedicated state channel to track iterations across resume cycles: index:1-50 

Add to your agent state:
```python
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    handoff_requested: bool
    _refinement_state: dict  # Tracks iteration, feedback_history, previous_summary
    handoff_decision: dict
    handoff_approved: bool
```

## Metadata Tracking for LangSmith

For step-by-step iteration tracking visible in LangSmith, the task metadata system automatically tracks steps: index:1-50 

Each LLM call within your refinement loop will automatically get `langgraph_step` metadata. Augment this with your custom iteration metadata as shown above.

## Testing Pattern

Structure your integration tests following the DeepAgents HITL pattern: index:18-65 

Test multiple refinement iterations with different feedback strings, validating that:
1. Each iteration produces a new LLM call trace in LangSmith
2. Metadata correctly tracks iteration numbers
3. Feedback is incorporated into refined summaries
4. Final approval terminates the loop

## Summary of Implementation Strategy

1. **Middleware Loop**: Implement refinement loop in `HandoffSummarizationMiddleware.after_model` using the interrupt loop pattern
2. **Prompt Strategy**: Use refine chain pattern with `{existing_summary}` + `{feedback}` + `{messages}`
3. **Decision Structure**: Extend `EditDecision` with `feedback` field in `edited_action.args`
4. **State Tracking**: Store `_refinement_state` with iteration count and feedback history
5. **LangSmith Traceability**: Add metadata with `handoff_iteration`, `refinement_type`, and tags like `iteration_N`
6. **CLI Integration**: Add "Refine" option to `prompt_handoff_decision` that captures natural language feedback
7. **Unified Flows**: Route manual `/handoff` through middleware for consistency

This architecture is modular (middleware handles refinement logic, CLI handles UI), elegant (reuses existing HITL patterns), and provides excellent LangSmith visibility with clear iteration tracking and prompt evolution.

## Notes

The key architectural insight is that **refinement is a graph execution pattern, not a CLI-only concern**. By implementing the loop in middleware using LangGraph's `interrupt()` primitive, you get:
- Automatic checkpointing between iterations
- Resume semantics that work across process boundaries
- Full LangSmith tracing with parent-child run relationships
- Consistency between agent-initiated and manual handoff flows

The refine chain pattern from LangChain provides the proven prompt structure for incorporating feedback, while the HITL decision types provide the protocol for communicating refinement requests between CLI and middleware.


### Citations


**File:** langgraph-main/libs/langgraph/tests/test_pregel.py (L4875-4892)

```python
def test_interrupt_loop(sync_checkpointer: BaseCheckpointSaver):
    class State(TypedDict):
        age: int
        other: str

    def ask_age(s: State):
        """Ask an expert for help."""
        question = "How old are you?"
        value = None
        for _ in range(10):
            value: str = interrupt(question)
            if not value.isdigit() or int(value) < 18:
                question = "invalid response"
                value = None
            else:
                break

        return {"age": int(value)}
```

**File:** langgraph-main/libs/langgraph/langgraph/types.py (L401-422)

```python
def interrupt(value: Any) -> Any:
    """Interrupt the graph with a resumable exception from within a node.

    The `interrupt` function enables human-in-the-loop workflows by pausing graph
    execution and surfacing a value to the client. This value can communicate context
    or request input required to resume execution.

    In a given node, the first invocation of this function raises a `GraphInterrupt`
    exception, halting execution. The provided `value` is included with the exception
    and sent to the client executing the graph.

    A client resuming the graph must use the [`Command`][langgraph.types.Command]
    primitive to specify a value for the interrupt and continue execution.
    The graph resumes from the start of the node, **re-executing** all logic.

    If a node contains multiple `interrupt` calls, LangGraph matches resume values
    to interrupts based on their order in the node. This list of resume values
    is scoped to the specific task executing the node and is not shared across tasks.

    To use an `interrupt`, you must enable a checkpointer, as the feature relies
    on persisting the graph state.

```

**File:** langchain-master/libs/langchain/langchain_classic/chains/summarize/refine_prompts.py (L3-13)

```python
REFINE_PROMPT_TMPL = """\
Your job is to produce a final summary.
We have provided an existing summary up to a certain point: {existing_answer}
We have the opportunity to refine the existing summary (only if needed) with some more context below.
------------
{text}
------------
Given the new context, refine the original summary.
If the context isn't useful, return the original summary.\
"""  # noqa: E501
REFINE_PROMPT = PromptTemplate.from_template(REFINE_PROMPT_TMPL)
```

**File:** langchain-master/libs/langchain/langchain_classic/chains/llm_summarization_checker/base.py (L78-108)

```python
class LLMSummarizationCheckerChain(Chain):
    """Chain for question-answering with self-verification.

    Example:
        ```python
        from langchain_openai import OpenAI
        from langchain_classic.chains import LLMSummarizationCheckerChain

        model = OpenAI(temperature=0.0)
        checker_chain = LLMSummarizationCheckerChain.from_llm(model)
        ```
    """

    sequential_chain: SequentialChain
    llm: BaseLanguageModel | None = None
    """[Deprecated] LLM wrapper to use."""

    create_assertions_prompt: PromptTemplate = CREATE_ASSERTIONS_PROMPT
    """[Deprecated]"""
    check_assertions_prompt: PromptTemplate = CHECK_ASSERTIONS_PROMPT
    """[Deprecated]"""
    revised_summary_prompt: PromptTemplate = REVISED_SUMMARY_PROMPT
    """[Deprecated]"""
    are_all_true_prompt: PromptTemplate = ARE_ALL_TRUE_PROMPT
    """[Deprecated]"""

    input_key: str = "query"
    output_key: str = "result"
    max_checks: int = 2
    """Maximum number of times to check the assertions. Default to double-checking."""

```

**File:** langchain-master/libs/langchain_v1/langchain/agents/middleware/human_in_the_loop.py (L36-92)

```python
DecisionType = Literal["approve", "edit", "reject"]


class ReviewConfig(TypedDict):
    """Policy for reviewing a HITL request."""

    action_name: str
    """Name of the action associated with this review configuration."""

    allowed_decisions: list[DecisionType]
    """The decisions that are allowed for this request."""

    args_schema: NotRequired[dict[str, Any]]
    """JSON schema for the args associated with the action, if edits are allowed."""


class HITLRequest(TypedDict):
    """Request for human feedback on a sequence of actions requested by a model."""

    action_requests: list[ActionRequest]
    """A list of agent actions for human review."""

    review_configs: list[ReviewConfig]
    """Review configuration for all possible actions."""


class ApproveDecision(TypedDict):
    """Response when a human approves the action."""

    type: Literal["approve"]
    """The type of response when a human approves the action."""


class EditDecision(TypedDict):
    """Response when a human edits the action."""

    type: Literal["edit"]
    """The type of response when a human edits the action."""

    edited_action: Action
    """Edited action for the agent to perform.

    Ex: for a tool call, a human reviewer can edit the tool name and args.
    """


class RejectDecision(TypedDict):
    """Response when a human rejects the action."""

    type: Literal["reject"]
    """The type of response when a human rejects the action."""

    message: NotRequired[str]
    """The message sent to the model explaining why the action was rejected."""


Decision = ApproveDecision | EditDecision | RejectDecision
```

**File:** langsmith-sdk-main/python/langsmith/run_helpers.py (L56-93)

```python
_CONTEXT_KEYS: dict[str, contextvars.ContextVar] = {
    "parent": _context._PARENT_RUN_TREE,
    "project_name": _context._PROJECT_NAME,
    "tags": _context._TAGS,
    "metadata": _context._METADATA,
    "enabled": _context._TRACING_ENABLED,
    "client": _context._CLIENT,
    "replicas": run_trees._REPLICAS,
    "distributed_parent_id": run_trees._DISTRIBUTED_PARENT_ID,
}

_EXCLUDED_FRAME_FNAME = "langsmith/run_helpers.py"

_OTEL_AVAILABLE: Optional[bool] = None


def get_current_run_tree() -> Optional[run_trees.RunTree]:
    """Get the current run tree."""
    return _context._PARENT_RUN_TREE.get()


def get_tracing_context(
    context: Optional[contextvars.Context] = None,
) -> dict[str, Any]:
    """Get the current tracing context."""
    if context is None:
        return {
            "parent": _context._PARENT_RUN_TREE.get(),
            "project_name": _context._PROJECT_NAME.get(),
            "tags": _context._TAGS.get(),
            "metadata": _context._METADATA.get(),
            "enabled": _context._TRACING_ENABLED.get(),
            "client": _context._CLIENT.get(),
            "replicas": run_trees._REPLICAS.get(),
            "distributed_parent_id": run_trees._DISTRIBUTED_PARENT_ID.get(),
        }
    return {k: context.get(v) for k, v in _CONTEXT_KEYS.items()}

```

**File:** langsmith-docs-main/docs/observability/how_to_guides/add_metadata_tags.mdx (L1-50)

```text
---
sidebar_position: 5
---

import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
} from "@site/src/components/InstructionsWithCode";

# Add metadata and tags to traces

LangSmith supports sending arbitrary metadata and tags along with traces.

Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.

Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID.
For more information on tags and metadata, see the [Concepts](/observability/concepts#tags) page. For information on how to query traces and runs by metadata and tags, see the [Filter traces in the application](./filter_traces_in_application) page.

<CodeTabs
  tabs={[
    PythonBlock(`import openai
import langsmith as ls
from langsmith.wrappers import wrap_openai
client = openai.Client()\n
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]\n
# highlight-next-line
# You can set metadata & tags **statically** when decorating a function
# Use the @traceable decorator with tags and metadata
# Ensure that the LANGSMITH_TRACING environment variables are set for @traceable to work
@ls.traceable(
    run_type="llm",
    name="OpenAI Call Decorator",
    # highlight-next-line
    tags=["my-tag"],
    # highlight-next-line
    metadata={"my-key": "my-value"}
)
def call_openai(
    messages: list[dict], model: str = "gpt-4o-mini"
) -> str:
    # You can also dynamically set metadata on the parent run:
    # highlight-next-line
    rt = ls.get_current_run_tree()
    # highlight-next-line
    rt.metadata["some-conditional-key"] = "some-val"
    # highlight-next-line
```

**File:** langgraph-main/docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md (L20-86)

```markdown
## Pause using `interrupt`

:::python
[Dynamic interrupts](../../concepts/human_in_the_loop.md#key-capabilities) (also known as dynamic breakpoints) are triggered based on the current state of the graph. You can set dynamic interrupts by calling @[`interrupt` function][interrupt] in the appropriate place. The graph will pause, which allows for human intervention, and then resumes the graph with their input. It's useful for tasks like approvals, edits, or gathering additional context.

!!! note

    As of v1.0, `interrupt` is the recommended way to pause a graph. `NodeInterrupt` is deprecated and will be removed in v2.0.

:::

:::js
[Dynamic interrupts](../../concepts/human_in_the_loop.md#key-capabilities) (also known as dynamic breakpoints) are triggered based on the current state of the graph. You can set dynamic interrupts by calling @[`interrupt` function][interrupt] in the appropriate place. The graph will pause, which allows for human intervention, and then resumes the graph with their input. It's useful for tasks like approvals, edits, or gathering additional context.
:::

To use `interrupt` in your graph, you need to:

1. [**Specify a checkpointer**](../../concepts/persistence.md#checkpoints) to save the graph state after each step.
2. **Call `interrupt()`** in the appropriate place. See the [Common Patterns](#common-patterns) section for examples.
3. **Run the graph** with a [**thread ID**](../../concepts/persistence.md#threads) until the `interrupt` is hit.
4. **Resume execution** using `invoke`/`stream` (see [**The `Command` primitive**](#resume-using-the-command-primitive)).

:::python

```python
# highlight-next-line
from langgraph.types import interrupt, Command

def human_node(state: State):
    # highlight-next-line
    value = interrupt( # (1)!
        {
            "text_to_revise": state["some_text"] # (2)!
        }
    )
    return {
        "some_text": value # (3)!
    }


graph = graph_builder.compile(checkpointer=checkpointer) # (4)!

# Run the graph until the interrupt is hit.
config = {"configurable": {"thread_id": "some_id"}}
result = graph.invoke({"some_text": "original text"}, config=config) # (5)!
print(result['__interrupt__']) # (6)!
# > [
# >    Interrupt(
# >       value={'text_to_revise': 'original text'},
# >       resumable=True,
# >       ns=['human_node:6ce9e64f-edef-fe5d-f7dc-511fa9526960']
# >    )
# > ]

# highlight-next-line
print(graph.invoke(Command(resume="Edited text"), config=config)) # (7)!
# > {'some_text': 'Edited text'}
```

1. `interrupt(...)` pauses execution at `human_node`, surfacing the given payload to a human.
2. Any JSON serializable value can be passed to the `interrupt` function. Here, a dict containing the text to revise.
3. Once resumed, the return value of `interrupt(...)` is the human-provided input, which is used to update the state.
4. A checkpointer is required to persist graph state. In production, this should be durable (e.g., backed by a database).
5. The graph is invoked with some initial state.
6. When the graph hits the interrupt, it returns an `Interrupt` object with the payload and metadata.
7. The graph is resumed with a `Command(resume=...)`, injecting the human's input and continuing execution.
   :::
```

**File:** langgraph-main/libs/langgraph/langgraph/graph/state.py (L1-50)

```python
from __future__ import annotations

import inspect
import logging
import typing
import warnings
from collections import defaultdict
from collections.abc import Awaitable, Callable, Hashable, Sequence
from functools import partial
from inspect import isclass, isfunction, ismethod, signature
from types import FunctionType
from types import NoneType as NoneType
from typing import (
    Any,
    Generic,
    Literal,
    Union,
    cast,
    get_args,
    get_origin,
    get_type_hints,
    overload,
)

from langchain_core.runnables import Runnable, RunnableConfig
from langgraph.cache.base import BaseCache
from langgraph.checkpoint.base import Checkpoint
from langgraph.store.base import BaseStore
from pydantic import BaseModel, TypeAdapter
from typing_extensions import NotRequired, Required, Self, Unpack, is_typeddict

from langgraph._internal._constants import (
    INTERRUPT,
    NS_END,
    NS_SEP,
    TASKS,
)
from langgraph._internal._fields import (
    get_cached_annotated_keys,
    get_field_default,
    get_update_as_tuples,
)
from langgraph._internal._pydantic import create_model
from langgraph._internal._runnable import coerce_to_runnable
from langgraph._internal._typing import EMPTY_SEQ, MISSING, DeprecatedKwargs
from langgraph.channels.base import BaseChannel
from langgraph.channels.binop import BinaryOperatorAggregate
from langgraph.channels.ephemeral_value import EphemeralValue
from langgraph.channels.last_value import LastValue, LastValueAfterFinish
from langgraph.channels.named_barrier_value import (
```

**File:** langgraph-main/libs/langgraph/langgraph/pregel/_algo.py (L1-50)

```python
from __future__ import annotations

import binascii
import itertools
import sys
import threading
from collections import defaultdict, deque
from collections.abc import Callable, Iterable, Mapping, Sequence
from copy import copy
from functools import partial
from hashlib import sha1
from typing import (
    Any,
    Literal,
    NamedTuple,
    Protocol,
    cast,
    overload,
)

from langchain_core.callbacks import Callbacks
from langchain_core.callbacks.manager import AsyncParentRunManager, ParentRunManager
from langchain_core.runnables.config import RunnableConfig
from langgraph.checkpoint.base import (
    BaseCheckpointSaver,
    ChannelVersions,
    Checkpoint,
    PendingWrite,
    V,
)
from langgraph.store.base import BaseStore
from xxhash import xxh3_128_hexdigest

from langgraph._internal._config import merge_configs, patch_config
from langgraph._internal._constants import (
    CACHE_NS_WRITES,
    CONF,
    CONFIG_KEY_CHECKPOINT_ID,
    CONFIG_KEY_CHECKPOINT_MAP,
    CONFIG_KEY_CHECKPOINT_NS,
    CONFIG_KEY_CHECKPOINTER,
    CONFIG_KEY_READ,
    CONFIG_KEY_RESUME_MAP,
    CONFIG_KEY_RUNTIME,
    CONFIG_KEY_SCRATCHPAD,
    CONFIG_KEY_SEND,
    CONFIG_KEY_TASK_ID,
    ERROR,
    INTERRUPT,
    NO_WRITES,
```

**File:** deepagents-master/libs/deepagents/tests/integration_tests/test_hitl.py (L18-65)

```python
    def test_hitl_agent(self):
        checkpointer = MemorySaver()
        agent = create_deep_agent(tools=[sample_tool, get_weather, get_soccer_scores], interrupt_on=SAMPLE_TOOL_CONFIG, checkpointer=checkpointer)
        config = {"configurable": {"thread_id": uuid.uuid4()}}
        assert_all_deepagent_qualities(agent)
        result = agent.invoke(
            {
                "messages": [
                    {
                        "role": "user",
                        "content": "Call the sample tool, get the weather in New York and get scores for the latest soccer games in parallel",
                    }
                ]
            },
            config=config,
        )
        agent_messages = [msg for msg in result.get("messages", []) if msg.type == "ai"]
        tool_calls = [tool_call for msg in agent_messages for tool_call in msg.tool_calls]
        assert any([tool_call["name"] == "sample_tool" for tool_call in tool_calls])
        assert any([tool_call["name"] == "get_weather" for tool_call in tool_calls])
        assert any([tool_call["name"] == "get_soccer_scores" for tool_call in tool_calls])

        assert result["__interrupt__"] is not None
        interrupts = result["__interrupt__"][0].value
        action_requests = interrupts["action_requests"]
        assert len(interrupts) == 2
        assert any([action_request["name"] == "sample_tool" for action_request in action_requests])
        assert any([action_request["name"] == "get_soccer_scores" for action_request in action_requests])
        review_configs = interrupts["review_configs"]
        assert any(
            [
                review_config["action_name"] == "sample_tool" and review_config["allowed_decisions"] == ["approve", "edit", "reject"]
                for review_config in review_configs
            ]
        )
        assert any(
            [
                review_config["action_name"] == "get_soccer_scores" and review_config["allowed_decisions"] == ["approve", "reject"]
                for review_config in review_configs
            ]
        )

        result2 = agent.invoke(Command(resume={"decisions": [{"type": "approve"}, {"type": "approve"}]}), config=config)
        tool_results = [msg for msg in result2.get("messages", []) if msg.type == "tool"]
        assert any([tool_result.name == "sample_tool" for tool_result in tool_results])
        assert any([tool_result.name == "get_weather" for tool_result in tool_results])
        assert any([tool_result.name == "get_soccer_scores" for tool_result in tool_results])
        assert "__interrupt__" not in result2
```

